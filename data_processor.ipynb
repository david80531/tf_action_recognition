{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "from tf_pose.estimator import BodyPart\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-27 15:35:13,390] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('TfPoseEstimator-Video')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "fps_time = 0\n",
    "num_class = 9\n",
    "video_dict = { 'PlayingCello':0,'PlayingDaf':1,\n",
    "                  'PlayingDhol':2,'PlayingFlute':3,\n",
    "                  'PlayingGuitar':4,'PlayingPiano':5\n",
    "                  , 'PlayingSitar':6,'PlayingTabla':7,'PlayingViolin':8}\n",
    "inv_video_dict = {v: k for k, v in video_dict.items()}\n",
    "model_path='mobilenet_thin'\n",
    "resolution = '320x240'\n",
    "showBG=True\n",
    "w, h = model_wh(resolution)\n",
    "e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_human_data(humans):\n",
    "    \n",
    "    if (len(humans)==0):\n",
    "        return np.zeros(shape=(18,2))\n",
    "                        \n",
    "    feature = np.zeros(shape=(18,2))\n",
    "    for i in range(18):\n",
    "        if i not in humans[0].body_parts:\n",
    "            feature[i] = [0, 0]\n",
    "        else:\n",
    "            feature[i] = [humans[0].body_parts[i].x, humans[0].body_parts[i].y]\n",
    "    \n",
    "    return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index_range(start, end):\n",
    "    if(start > end):\n",
    "        raise ValueError('start index must be smaller than end index')\n",
    "    elif(start < 0):\n",
    "        raise ValueError('start index cannot be negative')\n",
    "    elif(end>=num_class):\n",
    "        raise ValueError('end index is out of range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile(precision=4)  uncomment to show memory info\n",
    "def inference_video(path):\n",
    "    logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    #---------------modified----------------#\n",
    "    num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print (\"All Frames: \" ,num_frames)\n",
    "    cur_frames = 0.0\n",
    "    step = (num_frames / 20.0) \n",
    "    #---------------modified----------------#\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    resize_out_ratio = 8.0\n",
    "    #print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "    single_video_features = np.array([])\n",
    "    if cap.isOpened() is False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    while (cap.isOpened()):   \n",
    "        if(cur_frames >= num_frames):\n",
    "            break\n",
    "\n",
    "        frame_no = (cur_frames/num_frames)\n",
    "        cap.set(1,frame_no)\n",
    "        ret_val, image = cap.read()\n",
    "\n",
    "        #print(\"Frame no: \", frame_no)\n",
    "        #print (\"Count: \", cur_frames)\n",
    "\n",
    "        if ret_val == True:\n",
    "            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "            #print (\"Frame numbers: \", cur_frames, humans)\n",
    "            frame_feature = process_human_data(humans) \n",
    "            single_video_features = np.append(single_video_features, frame_feature) \n",
    "\n",
    "        cur_frames+=step\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    #print (single_video_features)\n",
    "    cv2.destroyAllWindows()\n",
    "    #logger.debug('finished+')\n",
    "    cap.release()\n",
    "    return single_video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(classname):\n",
    "    label=np.zeros(shape=(9))\n",
    "    label[video_dict[classname]]=1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@profile(precision=4)  uncomment to show memory info\n",
    "def create_training_data(start, end):\n",
    "    check_index_range(start, end)\n",
    "    mypath = Path().absolute()\n",
    "    dataset_path = os.path.abspath(os.path.join(str(mypath), os.pardir))+\"/action_dataset\"\n",
    "\n",
    "    feature_set=[]\n",
    "\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        finished_dirc = 0\n",
    "        for dirss in dirs:\n",
    "            finished_video = 0\n",
    "            if (dirss in video_dict and(video_dict[dirss]>=start and video_dict[dirss]<=end)):\n",
    "                for filename in os.listdir(os.path.join(subdir,dirss)):\n",
    "                    abs_path =os.path.join(subdir,dirss,filename)\n",
    "                    print(\"inferencing video: \", filename, \" from \" , dirss, \" directory\")\n",
    "                    feature =inference_video(abs_path)\n",
    "                    classification = get_classification(dirss)\n",
    "                    feature =list(feature)\n",
    "                    feature_set.append([feature,classification])\n",
    "                    print(\"video \", filename, \" inferenced done.\")\n",
    "                    \n",
    "                    print(\"# of finished videos: \", finished_video+1)\n",
    "                    print(\"# of finished directories: \", finished_dirc)\n",
    "                    finished_video+=1\n",
    "                finished_dirc += 1\n",
    "    pickle_file_name = 'feature_set_class_'+str(start)+'_to'+str(end)+'.pickle'\n",
    "    print('writing data to pickle files.....')\n",
    "    with open(pickle_file_name,'wb') as file:\n",
    "        pickle.dump(feature_set,file)\n",
    "    return pickle_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dict():\n",
    "    return video_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video_dict():\n",
    "    inv_vd_dic = {v: k for k, v in video_dict.items()}\n",
    "    print(\"ClassNumber          Class\")\n",
    "    print(\"----------------------------------\")\n",
    "    for i in range(len(video_dict)):\n",
    "        print(i,\"               \",inv_vd_dic[i])\n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
