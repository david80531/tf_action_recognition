{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "from tf_pose.estimator import BodyPart\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 17:10:01,811] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('TfPoseEstimator-Video')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "fps_time = 0\n",
    "video_dict = { 'PlayingCello':0,'PlayingDaf':1,\n",
    "                  'PlayingDhol':2,'PlayingFlute':3,\n",
    "                  'PlayingGuitar':4,'PlayingPiano':5\n",
    "                  , 'PlayingSitar':6,'PlayingTabla':7,'PlayingViolin':8}\n",
    "model_path='mobilenet_thin'\n",
    "resolution = '320x240'\n",
    "showBG=True\n",
    "w, h = model_wh(resolution)\n",
    "e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_human_data(humans):\n",
    "    \n",
    "    if (len(humans)==0):\n",
    "        return np.zeros(shape=(18,2))\n",
    "                        \n",
    "    feature = np.zeros(shape=(18,2))\n",
    "    for i in range(18):\n",
    "        if i not in humans[0].body_parts:\n",
    "            feature[i] = [0, 0]\n",
    "        else:\n",
    "            feature[i] = [humans[0].body_parts[i].x, humans[0].body_parts[i].y]\n",
    "    \n",
    "    return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile(precision=4)\n",
    "def inference_video(path):\n",
    "    logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    #---------------modified----------------#\n",
    "    num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print (\"All Frames: \" ,num_frames)\n",
    "    cur_frames = 0.0\n",
    "    step = (num_frames / 20.0) \n",
    "    #---------------modified----------------#\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    resize_out_ratio = 8.0\n",
    "    #print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "    single_video_features = np.array([])\n",
    "    if cap.isOpened() is False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    while (cap.isOpened()):   \n",
    "        if(cur_frames >= num_frames):\n",
    "            break\n",
    "\n",
    "        frame_no = (cur_frames/num_frames)\n",
    "        cap.set(1,frame_no)\n",
    "        ret_val, image = cap.read()\n",
    "\n",
    "        #print(\"Frame no: \", frame_no)\n",
    "        #print (\"Count: \", cur_frames)\n",
    "\n",
    "        if ret_val == True:\n",
    "            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "            #print (\"Frame numbers: \", cur_frames, humans)\n",
    "            frame_feature = process_human_data(humans) \n",
    "            single_video_features = np.append(single_video_features, frame_feature) \n",
    "\n",
    "        cur_frames+=step\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    #print (single_video_features)\n",
    "    cv2.destroyAllWindows()\n",
    "    #logger.debug('finished+')\n",
    "    cap.release()\n",
    "    return single_video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(filename):\n",
    "    label=np.zeros(shape=(9))\n",
    "    label[video_dict[filename]]=1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_memory_usage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**20  # memory use in MB...I think\n",
    "    print('memory use:', memoryUse, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@profile(precision=4)\n",
    "def create_traing_data():\n",
    "    mypath = Path().absolute()\n",
    "    dataset_path = os.path.abspath(os.path.join(mypath, os.pardir))+\"/action_dataset\"\n",
    "\n",
    "    feature_set=[]\n",
    "\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        for dirss in dirs:\n",
    "            finished_video = 0\n",
    "            if (dirss in video_dict):\n",
    "                for filename in os.listdir(os.path.join(subdir,dirss)):\n",
    "                    abs_path =os.path.join(subdir,dirss,filename)\n",
    "                    #show_memory_usage()\n",
    "                    feature =inference_video(abs_path)\n",
    "                    #show_memory_usage()\n",
    "                    classification = get_classification(dirss)\n",
    "                    feature =list(feature)\n",
    "                    feature_set.append([feature,classification])\n",
    "                    print(\"Finish video: \", finished_video+1)\n",
    "                    #print(\"Video \", finished_video,\" features map: \", feature_set[finished_video])\n",
    "                    finished_video+=1\n",
    "    with open('feature_set_c_4_5.pickle','wb') as file:\n",
    "        pickle.dump(feature_set,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
