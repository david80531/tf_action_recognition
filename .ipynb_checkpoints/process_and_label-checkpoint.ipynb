{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tf_pose.estimator import BodyPart\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TfPoseEstimator-Video')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "fps_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_human_data(humans):\n",
    "    \n",
    "    if (len(humans)==0):\n",
    "        return np.zeros(shape=(18,2))\n",
    "                        \n",
    "    feature = np.zeros(shape=(18,2))\n",
    "    for i in range(18):\n",
    "        if i not in humans[0].body_parts:\n",
    "            feature[i] = [0, 0]\n",
    "        else:\n",
    "            feature[i] = [humans[0].body_parts[i].x, humans[0].body_parts[i].y]\n",
    "    \n",
    "    return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 15:20:37,462] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 15:20:37,463] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Frames:  225.0\n",
      "Image Size: 320 x 240\n",
      "Frame no:  0.0\n",
      "Count:  0.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.05\n",
      "Count:  11.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.1\n",
      "Count:  22.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.15\n",
      "Count:  33.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.2\n",
      "Count:  45.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.25\n",
      "Count:  56.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.3\n",
      "Count:  67.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.35\n",
      "Count:  78.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.4\n",
      "Count:  90.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.45\n",
      "Count:  101.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.5\n",
      "Count:  112.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.55\n",
      "Count:  123.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.6\n",
      "Count:  135.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.65\n",
      "Count:  146.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.7\n",
      "Count:  157.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.75\n",
      "Count:  168.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.8\n",
      "Count:  180.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.85\n",
      "Count:  191.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.9\n",
      "Count:  202.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.95\n",
      "Count:  213.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 15:21:01,194] [TfPoseEstimator-Video] [DEBUG] finished+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "(720,)\n"
     ]
    }
   ],
   "source": [
    "model_path='mobilenet_thin'\n",
    "resolution = '320x240'\n",
    "showBG=True\n",
    "    \n",
    "logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "w, h = model_wh(resolution)\n",
    "e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
    "\n",
    "\n",
    "video = '../UCF-101/PlayingPiano/v_PlayingPiano_g02_c01.avi'\n",
    "cap = cv2.VideoCapture(video)\n",
    "\n",
    "#---------------modified----------------#\n",
    "num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print (\"All Frames: \" ,num_frames)\n",
    "cur_frames = 0.0\n",
    "step = (num_frames / 20.0) \n",
    "#---------------modified----------------#\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "resize_out_ratio = 8.0\n",
    "print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "single_video_features = np.array([])\n",
    "if cap.isOpened() is False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "while (cap.isOpened()):   \n",
    "    if(cur_frames >= num_frames):\n",
    "        break\n",
    "\n",
    "    frame_no = (cur_frames/num_frames)\n",
    "    cap.set(1,frame_no)\n",
    "    ret_val, image = cap.read()\n",
    "    \n",
    "    print(\"Frame no: \", frame_no)\n",
    "    print (\"Count: \", cur_frames)\n",
    "    \n",
    "    if ret_val == True:\n",
    "        humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "        #print (\"Frame numbers: \", cur_frames, humans)\n",
    "        frame_feature = process_human_data(humans) \n",
    "        single_video_features = np.append(single_video_features, frame_feature) \n",
    "        \n",
    "    cur_frames+=step\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "print (single_video_features)\n",
    "cv2.destroyAllWindows()\n",
    "logger.debug('finished+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.475      0.22916667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature = np.zeros(shape=(18,2))\n",
    "for i in range(18):\n",
    "    if i not in humans[0].body_parts:\n",
    "        feature[i] = [0, 0]\n",
    "    else:\n",
    "        feature[i] = [bp[i].x, bp[i].y]\n",
    "feature = list(feature)\n",
    "\n",
    "print(feature[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:04:56,175] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:04:56,176] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n",
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.48, 0.20) score=0.86\n",
      "human_id= 0 , BodyPart:1-(0.54, 0.28) score=0.76\n",
      "human_id= 0 , BodyPart:2-(0.48, 0.27) score=0.70\n",
      "human_id= 0 , BodyPart:3-(0.41, 0.37) score=0.56\n",
      "human_id= 0 , BodyPart:4-(0.36, 0.41) score=0.67\n",
      "human_id= 0 , BodyPart:5-(0.62, 0.28) score=0.63\n",
      "human_id= 0 , BodyPart:6-(0.68, 0.43) score=0.77\n",
      "human_id= 0 , BodyPart:7-(0.62, 0.33) score=0.50\n",
      "human_id= 0 , BodyPart:8-(0.48, 0.58) score=0.43\n",
      "human_id= 0 , BodyPart:9-(0.46, 0.74) score=0.10\n",
      "human_id= 0 , BodyPart:10-(0.44, 0.85) score=0.12\n",
      "human_id= 0 , BodyPart:11-(0.60, 0.58) score=0.65\n",
      "human_id= 0 , BodyPart:12-(0.54, 0.75) score=0.81\n",
      "human_id= 0 , BodyPart:13-(0.57, 0.95) score=0.19\n",
      "human_id= 0 , BodyPart:14-(0.48, 0.17) score=0.73\n",
      "human_id= 0 , BodyPart:15-(0.50, 0.18) score=0.85\n",
      "human_id= 0 , BodyPart:17-(0.55, 0.17) score=0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e3fe6fdcadad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_set.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0miterate_interest_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e3fe6fdcadad>\u001b[0m in \u001b[0;36miterate_interest_dir\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mabs_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minference_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f3d877a9476>\u001b[0m in \u001b[0;36minference_video\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mhumans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_to_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_out_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;31m#print (\"Frame numbers: \", cur_frames, humans)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mframe_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_human_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhumans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_action_recognition/tf_pose/estimator.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, npimg, resize_to_default, upsample_size)\u001b[0m\n\u001b[1;32m    366\u001b[0m         peaks, heatMat_up, pafMat_up = self.persistent_sess.run(\n\u001b[1;32m    367\u001b[0m             [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up], feed_dict={\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_image\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsample_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             })\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m#peaks= (1, 300, 500, 19)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "video_dict = { 'PlayingCello':0,'PlayingDaf':1,'PlayingDhol':2,'PlayingFlute':3,'PlayingGuitar':4,'PlayingPiano':5, 'PlayingSitar':6,'PlayingTabla':7,'PlayingViolin':8}\n",
    "rootdir = '/home/MPLab/mplab006/UCF-101/'\n",
    "def iterate_interest_dir():\n",
    "    feature_set=[]\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for dirss in dirs:\n",
    "            if (dirss in video_dict):\n",
    "                for filename in os.listdir(os.path.join(subdir,dirss)):\n",
    "                    abs_path =os.path.join(subdir,dirss,filename)\n",
    "                    feature =inference_video(abs_path)\n",
    "                    classification = get_classification(dirss)\n",
    "                    feature =list(feature)\n",
    "                    print(feature)\n",
    "                    feature_set.append([feature,classification])\n",
    "                    #print(feature_set)\n",
    "    with open('feature_set.pickle','wb') as file:\n",
    "        pickle.dump(feature_set,file)\n",
    "iterate_interest_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(filename):\n",
    "    label=np.zeros(shape=(9))\n",
    "    label[video_dict[filename]]=1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_video(path):\n",
    "    model_path='mobilenet_thin'\n",
    "    resolution = '320x240'\n",
    "    showBG=True\n",
    "\n",
    "    logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "    w, h = model_wh(resolution)\n",
    "    e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    #---------------modified----------------#\n",
    "    num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print (\"All Frames: \" ,num_frames)\n",
    "    cur_frames = 0.0\n",
    "    step = (num_frames / 20.0) \n",
    "    #---------------modified----------------#\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    resize_out_ratio = 8.0\n",
    "    #print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "    single_video_features = np.array([])\n",
    "    if cap.isOpened() is False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    while (cap.isOpened()):   \n",
    "        if(cur_frames >= num_frames):\n",
    "            break\n",
    "\n",
    "        frame_no = (cur_frames/num_frames)\n",
    "        cap.set(1,frame_no)\n",
    "        ret_val, image = cap.read()\n",
    "\n",
    "        #print(\"Frame no: \", frame_no)\n",
    "        #print (\"Count: \", cur_frames)\n",
    "\n",
    "        if ret_val == True:\n",
    "            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "            #print (\"Frame numbers: \", cur_frames, humans)\n",
    "            frame_feature = process_human_data(humans) \n",
    "            single_video_features = np.append(single_video_features, frame_feature) \n",
    "\n",
    "        cur_frames+=step\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    #print (single_video_features)\n",
    "    cv2.destroyAllWindows()\n",
    "    #logger.debug('finished+')\n",
    "    return single_video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
