{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "from tf_pose.estimator import BodyPart\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TfPoseEstimator-Video')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "fps_time = 0\n",
    "video_dict = { 'PlayingCello':0,'PlayingDaf':1,\n",
    "                  'PlayingDhol':2,'PlayingFlute':3,\n",
    "                  'PlayingGuitar':4,'PlayingPiano':5\n",
    "                  , 'PlayingSitar':6,'PlayingTabla':7,'PlayingViolin':8}\n",
    "model_path='mobilenet_thin'\n",
    "resolution = '320x240'\n",
    "showBG=True\n",
    "w, h = model_wh(resolution)\n",
    "e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_human_data(humans):\n",
    "    \n",
    "    if (len(humans)==0):\n",
    "        return np.zeros(shape=(18,2))\n",
    "                        \n",
    "    feature = np.zeros(shape=(18,2))\n",
    "    for i in range(18):\n",
    "        if i not in humans[0].body_parts:\n",
    "            feature[i] = [0, 0]\n",
    "        else:\n",
    "            feature[i] = [humans[0].body_parts[i].x, humans[0].body_parts[i].y]\n",
    "    \n",
    "    return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile(precision=4)\n",
    "def inference_video(path):\n",
    "    logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    #---------------modified----------------#\n",
    "    num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print (\"All Frames: \" ,num_frames)\n",
    "    cur_frames = 0.0\n",
    "    step = (num_frames / 20.0) \n",
    "    #---------------modified----------------#\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    resize_out_ratio = 8.0\n",
    "    #print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "    single_video_features = np.array([])\n",
    "    if cap.isOpened() is False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    while (cap.isOpened()):   \n",
    "        if(cur_frames >= num_frames):\n",
    "            break\n",
    "\n",
    "        frame_no = (cur_frames/num_frames)\n",
    "        cap.set(1,frame_no)\n",
    "        ret_val, image = cap.read()\n",
    "\n",
    "        #print(\"Frame no: \", frame_no)\n",
    "        #print (\"Count: \", cur_frames)\n",
    "\n",
    "        if ret_val == True:\n",
    "            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "            #print (\"Frame numbers: \", cur_frames, humans)\n",
    "            frame_feature = process_human_data(humans) \n",
    "            single_video_features = np.append(single_video_features, frame_feature) \n",
    "\n",
    "        cur_frames+=step\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    #print (single_video_features)\n",
    "    cv2.destroyAllWindows()\n",
    "    #logger.debug('finished+')\n",
    "    cap.release()\n",
    "    return single_video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(filename):\n",
    "    label=np.zeros(shape=(9))\n",
    "    label[video_dict[filename]]=1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_memory_usage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**20  # memory use in MB...I think\n",
    "    print('memory use:', memoryUse, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:01:51,830] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n",
      "[2018-07-26 15:02:08,924] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 228.4062 MiB 228.4062 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 228.4062 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 228.4062 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 228.4062 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 228.4062 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 310.7969 MiB  82.3906 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 312.6641 MiB   1.8672 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 312.6641 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 312.6641 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 312.6641 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 312.6758 MiB   0.0117 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 312.6758 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 312.6758 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 312.6758 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 312.6758 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 312.6758 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 371.6406 MiB -50.5234 MiB       while (cap.isOpened()):   \n",
      "    71 371.6406 MiB -50.5234 MiB           if(cur_frames >= num_frames):\n",
      "    72 369.1875 MiB  -2.4531 MiB               break\n",
      "    73                             \n",
      "    74 371.6406 MiB -48.0703 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 371.6406 MiB -47.9648 MiB           cap.set(1,frame_no)\n",
      "    76 371.6406 MiB -47.8633 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 371.6406 MiB -48.0703 MiB           if ret_val == True:\n",
      "    82 371.6367 MiB   8.1016 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 371.6367 MiB -50.4570 MiB               frame_feature = process_human_data(humans) \n",
      "    85 371.6406 MiB -50.4414 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 371.6406 MiB -50.5234 MiB           cur_frames+=step\n",
      "    88 371.6406 MiB -50.5117 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 369.1914 MiB   0.0039 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 367.3984 MiB  -1.7930 MiB       cap.release()\n",
      "    94 367.3984 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:02:27,084] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 302.6758 MiB 302.6758 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 302.6758 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 302.6758 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 302.6758 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 302.6758 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 353.2734 MiB  50.5977 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 353.3047 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 353.3047 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 353.3047 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 353.3047 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 353.3047 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 353.3047 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 353.3047 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 353.3047 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 353.3047 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 353.3047 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 411.9961 MiB -46.1758 MiB       while (cap.isOpened()):   \n",
      "    71 411.9961 MiB -46.1758 MiB           if(cur_frames >= num_frames):\n",
      "    72 409.0000 MiB  -2.9961 MiB               break\n",
      "    73                             \n",
      "    74 411.9961 MiB -43.1797 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 411.9961 MiB -43.1797 MiB           cap.set(1,frame_no)\n",
      "    76 411.9961 MiB -43.1797 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 411.9961 MiB -43.1797 MiB           if ret_val == True:\n",
      "    82 411.9961 MiB  12.5156 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 411.9961 MiB -46.1758 MiB               frame_feature = process_human_data(humans) \n",
      "    85 411.9961 MiB -46.1758 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 411.9961 MiB -46.1758 MiB           cur_frames+=step\n",
      "    88 411.9961 MiB -46.1758 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 409.0000 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 407.7969 MiB  -1.2031 MiB       cap.release()\n",
      "    94 407.7969 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:02:44,981] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 331.0273 MiB 331.0273 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 331.0273 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 331.0273 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 331.0273 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 331.0273 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 395.7656 MiB  64.7383 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 395.7969 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 395.7969 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 395.7969 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 395.7969 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 395.7969 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 395.7969 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 395.7969 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 395.7969 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 395.7969 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 395.7969 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 454.4688 MiB -48.8789 MiB       while (cap.isOpened()):   \n",
      "    71 454.4688 MiB -48.8789 MiB           if(cur_frames >= num_frames):\n",
      "    72 452.0156 MiB  -2.4531 MiB               break\n",
      "    73                             \n",
      "    74 454.4688 MiB -46.4258 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 454.4688 MiB -46.4258 MiB           cap.set(1,frame_no)\n",
      "    76 454.4688 MiB -46.4258 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 454.4688 MiB -46.4258 MiB           if ret_val == True:\n",
      "    82 454.4688 MiB   9.7930 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 454.4688 MiB -48.8789 MiB               frame_feature = process_human_data(humans) \n",
      "    85 454.4688 MiB -48.8789 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 454.4688 MiB -48.8789 MiB           cur_frames+=step\n",
      "    88 454.4688 MiB -48.8789 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 452.0156 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 450.2188 MiB  -1.7969 MiB       cap.release()\n",
      "    94 450.2188 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:03:02,997] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 364.5977 MiB 364.5977 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 364.5977 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 364.5977 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 364.5977 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 364.5977 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 425.0469 MiB  60.4492 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 425.0781 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 425.0781 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 425.0781 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 425.0781 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 425.0781 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 425.0781 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 425.0781 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 425.0781 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 425.0781 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 425.0781 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 486.5273 MiB -99.8359 MiB       while (cap.isOpened()):   \n",
      "    71 486.5273 MiB -99.8359 MiB           if(cur_frames >= num_frames):\n",
      "    72 481.2344 MiB  -5.2930 MiB               break\n",
      "    73                             \n",
      "    74 486.5273 MiB -94.5430 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 486.5273 MiB -94.5430 MiB           cap.set(1,frame_no)\n",
      "    76 486.5273 MiB -94.5430 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 486.5273 MiB -94.5430 MiB           if ret_val == True:\n",
      "    82 486.5273 MiB -38.3867 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 486.5273 MiB -99.8359 MiB               frame_feature = process_human_data(humans) \n",
      "    85 486.5273 MiB -99.8359 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 486.5273 MiB -99.8359 MiB           cur_frames+=step\n",
      "    88 486.5273 MiB -99.8359 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 481.2344 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 479.4453 MiB  -1.7891 MiB       cap.release()\n",
      "    94 479.4453 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:03:21,291] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 378.9648 MiB 378.9648 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 378.9648 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 378.9648 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 378.9648 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 378.9648 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 461.4922 MiB  82.5273 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 461.5234 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 461.5234 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 461.5234 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 461.5234 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 461.5234 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 461.5234 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 461.5234 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 461.5234 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 461.5234 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 461.5234 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 520.0430 MiB -34.8945 MiB       while (cap.isOpened()):   \n",
      "    71 520.0430 MiB -34.8945 MiB           if(cur_frames >= num_frames):\n",
      "    72 518.1406 MiB  -1.9023 MiB               break\n",
      "    73                             \n",
      "    74 520.0430 MiB -32.9922 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 520.0430 MiB -32.9922 MiB           cap.set(1,frame_no)\n",
      "    76 520.0430 MiB -32.9922 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 520.0430 MiB -32.9922 MiB           if ret_val == True:\n",
      "    82 520.0430 MiB  23.6250 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 520.0430 MiB -34.8945 MiB               frame_feature = process_human_data(humans) \n",
      "    85 520.0430 MiB -34.8945 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 520.0430 MiB -34.8945 MiB           cur_frames+=step\n",
      "    88 520.0430 MiB -34.8945 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 518.1406 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 516.3125 MiB  -1.8281 MiB       cap.release()\n",
      "    94 516.3125 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:03:40,959] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 406.8086 MiB 406.8086 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 406.8086 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 406.8086 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 406.8086 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 406.8086 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 500.7773 MiB  93.9688 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 500.8086 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 500.8086 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 500.8086 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 500.8086 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 500.8086 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 500.8086 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 500.8086 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 500.8086 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 500.8086 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 500.8086 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 559.2461 MiB -35.5430 MiB       while (cap.isOpened()):   \n",
      "    71 559.2461 MiB -35.5430 MiB           if(cur_frames >= num_frames):\n",
      "    72 557.9805 MiB  -1.2656 MiB               break\n",
      "    73                             \n",
      "    74 559.2461 MiB -34.2773 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 559.2461 MiB -34.2773 MiB           cap.set(1,frame_no)\n",
      "    76 559.2461 MiB -34.2773 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 559.2461 MiB -34.2773 MiB           if ret_val == True:\n",
      "    82 559.2461 MiB  22.8945 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 559.2461 MiB -35.5430 MiB               frame_feature = process_human_data(humans) \n",
      "    85 559.2461 MiB -35.5430 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 559.2461 MiB -35.5430 MiB           cur_frames+=step\n",
      "    88 559.2461 MiB -35.5430 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 557.9805 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 556.1914 MiB  -1.7891 MiB       cap.release()\n",
      "    94 556.1914 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-26 15:04:02,904] [TfPoseEstimator] [INFO] loading graph from /Users/david/Documents/system_implemetation/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/david/Documents/system_implemetation/tf_action_recognition/inference.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    40 430.8672 MiB 430.8672 MiB   @profile(precision=4)\n",
      "    41                             def inference_video_test(path):\n",
      "    42 430.8672 MiB   0.0000 MiB       model_path='mobilenet_thin'\n",
      "    43 430.8672 MiB   0.0000 MiB       resolution = '320x240'\n",
      "    44 430.8672 MiB   0.0000 MiB       showBG=True\n",
      "    45                             \n",
      "    46                                 #logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
      "    47 430.8672 MiB   0.0000 MiB       w, h = model_wh(resolution)\n",
      "    48 536.5312 MiB 105.6641 MiB       e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
      "    49                             \n",
      "    50                             \n",
      "    51 536.5625 MiB   0.0312 MiB       cap = cv2.VideoCapture(path)\n",
      "    52                             \n",
      "    53                                 #---------------modified----------------#\n",
      "    54 536.5625 MiB   0.0000 MiB       num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "    55                                 #print (\"All Frames: \" ,num_frames)\n",
      "    56 536.5625 MiB   0.0000 MiB       cur_frames = 0.0\n",
      "    57 536.5625 MiB   0.0000 MiB       step = (num_frames / 20.0) \n",
      "    58                                 #---------------modified----------------#\n",
      "    59                             \n",
      "    60 536.5625 MiB   0.0000 MiB       fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
      "    61 536.5625 MiB   0.0000 MiB       width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
      "    62 536.5625 MiB   0.0000 MiB       height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
      "    63 536.5625 MiB   0.0000 MiB       resize_out_ratio = 8.0\n",
      "    64                                 #print(\"Image Size: %d x %d\" % (width, height)) \n",
      "    65                             \n",
      "    66 536.5625 MiB   0.0000 MiB       single_video_features = np.array([])\n",
      "    67 536.5625 MiB   0.0000 MiB       if cap.isOpened() is False:\n",
      "    68                                     print(\"Error opening video stream or file\")\n",
      "    69                             \n",
      "    70 595.2891 MiB -51.8242 MiB       while (cap.isOpened()):   \n",
      "    71 595.2891 MiB -51.8242 MiB           if(cur_frames >= num_frames):\n",
      "    72 592.2422 MiB  -3.0469 MiB               break\n",
      "    73                             \n",
      "    74 595.2891 MiB -48.7773 MiB           frame_no = (cur_frames/num_frames)\n",
      "    75 595.2891 MiB -48.7773 MiB           cap.set(1,frame_no)\n",
      "    76 595.2891 MiB -48.7773 MiB           ret_val, image = cap.read()\n",
      "    77                             \n",
      "    78                                     #print(\"Frame no: \", frame_no)\n",
      "    79                                     #print (\"Count: \", cur_frames)\n",
      "    80                             \n",
      "    81 595.2891 MiB -48.7773 MiB           if ret_val == True:\n",
      "    82 595.2891 MiB   6.9023 MiB               humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
      "    83                                         #print (\"Frame numbers: \", cur_frames, humans)\n",
      "    84 595.2891 MiB -51.8242 MiB               frame_feature = process_human_data(humans) \n",
      "    85 595.2891 MiB -51.8242 MiB               single_video_features = np.append(single_video_features, frame_feature) \n",
      "    86                             \n",
      "    87 595.2891 MiB -51.8242 MiB           cur_frames+=step\n",
      "    88 595.2891 MiB -51.8242 MiB           if cv2.waitKey(1) == 27:\n",
      "    89                                         break\n",
      "    90                                 #print (single_video_features)\n",
      "    91 592.2422 MiB   0.0000 MiB       cv2.destroyAllWindows()\n",
      "    92                                 #logger.debug('finished+')\n",
      "    93 591.0391 MiB  -1.2031 MiB       cap.release()\n",
      "    94 591.0391 MiB   0.0000 MiB       return single_video_features\n",
      "\n",
      "\n",
      "Finish video:  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e24c8c941016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mabs_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;31m#show_memory_usage()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minference_video_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;31m#show_memory_usage()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/system_implemetation/tf_action_recognition/inference.py\u001b[0m in \u001b[0;36minference_video_test\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_wh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfPoseEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_graph_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/system_implemetation/tf_action_recognition/tf_pose/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph_path, target_size, tf_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TfPoseEstimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m       \u001b[0m_ProcessNewOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m# Treat input mappings that don't appear in the graph as an error, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36m_ProcessNewOps\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0mcolocation_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mnew_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_new_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0moriginal_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mnew_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_add_new_tf_operations\u001b[0;34m(self, compute_devices)\u001b[0m\n\u001b[1;32m   3538\u001b[0m     new_ops = [\n\u001b[1;32m   3539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3540\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3541\u001b[0m     ]\n\u001b[1;32m   3542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3538\u001b[0m     new_ops = [\n\u001b[1;32m   3539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3540\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3541\u001b[0m     ]\n\u001b[1;32m   3542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_from_tf_operation\u001b[0;34m(self, c_op, compute_device)\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names_in_use\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3434\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names_in_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3435\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3479\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_device_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3481\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_colocation_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3482\u001b[0m       \u001b[0mall_colocation_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3483\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcolocation_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_colocation_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3479\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_device_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3481\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_colocation_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3482\u001b[0m       \u001b[0mall_colocation_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3483\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcolocation_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_colocation_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mtrace_memory_usage\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_lineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_trace_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_trace_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@profile(precision=4)\n",
    "def create_traing_data():\n",
    "    mypath = Path().absolute()\n",
    "    dataset_path = os.path.abspath(os.path.join(mypath, os.pardir))+\"/action_dataset\"\n",
    "\n",
    "    feature_set=[]\n",
    "\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        for dirss in dirs:\n",
    "            finished_video = 0\n",
    "            if (dirss in video_dict and (video_dict[dirss]==4 or video_dict[dirss]==5)):\n",
    "                for filename in os.listdir(os.path.join(subdir,dirss)):\n",
    "                    abs_path =os.path.join(subdir,dirss,filename)\n",
    "                    #show_memory_usage()\n",
    "                    feature =inference_video(abs_path)\n",
    "                    #show_memory_usage()\n",
    "                    classification = get_classification(dirss)\n",
    "                    feature =list(feature)\n",
    "                    feature_set.append([feature,classification])\n",
    "                    print(\"Finish video: \", finished_video+1)\n",
    "                    print(\"Video \", finished_video,\" features map: \", feature_set[finished_video])\n",
    "                    finished_video+=1\n",
    "    with open('feature_set_c_4_5.pickle','wb') as file:\n",
    "        pickle.dump(feature_set,file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
