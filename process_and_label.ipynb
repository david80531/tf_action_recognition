{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tf_pose.estimator import BodyPart\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('TfPoseEstimator-Video')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "fps_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_human_data(humans):\n",
    "    \n",
    "    if (len(humans)==0):\n",
    "        return np.zeros(shape=(18,2))\n",
    "                        \n",
    "    feature = np.zeros(shape=(18,2))\n",
    "    for i in range(18):\n",
    "        if i not in humans[0].body_parts:\n",
    "            feature[i] = [0, 0]\n",
    "        else:\n",
    "            feature[i] = [humans[0].body_parts[i].x, humans[0].body_parts[i].y]\n",
    "    \n",
    "    return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 15:20:37,462] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 15:20:37,463] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Frames:  225.0\n",
      "Image Size: 320 x 240\n",
      "Frame no:  0.0\n",
      "Count:  0.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.05\n",
      "Count:  11.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.1\n",
      "Count:  22.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.15\n",
      "Count:  33.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.2\n",
      "Count:  45.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.25\n",
      "Count:  56.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.3\n",
      "Count:  67.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.35\n",
      "Count:  78.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.4\n",
      "Count:  90.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.45\n",
      "Count:  101.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.5\n",
      "Count:  112.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.55\n",
      "Count:  123.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.6\n",
      "Count:  135.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.65\n",
      "Count:  146.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.7\n",
      "Count:  157.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.75\n",
      "Count:  168.75\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.8\n",
      "Count:  180.0\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.85\n",
      "Count:  191.25\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.9\n",
      "Count:  202.5\n",
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "Frame no:  0.95\n",
      "Count:  213.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 15:21:01,194] [TfPoseEstimator-Video] [DEBUG] finished+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_id= 0 , BodyPart:0-(0.71, 0.54) score=0.60\n",
      "human_id= 0 , BodyPart:1-(0.73, 0.60) score=0.53\n",
      "human_id= 0 , BodyPart:2-(0.72, 0.60) score=0.45\n",
      "human_id= 0 , BodyPart:3-(0.69, 0.66) score=0.16\n",
      "human_id= 0 , BodyPart:5-(0.76, 0.59) score=0.66\n",
      "human_id= 0 , BodyPart:6-(0.75, 0.68) score=0.50\n",
      "human_id= 0 , BodyPart:7-(0.69, 0.72) score=0.58\n",
      "human_id= 0 , BodyPart:8-(0.71, 0.74) score=0.26\n",
      "human_id= 0 , BodyPart:9-(0.69, 0.79) score=0.07\n",
      "human_id= 0 , BodyPart:11-(0.75, 0.74) score=0.33\n",
      "human_id= 0 , BodyPart:12-(0.71, 0.77) score=0.12\n",
      "human_id= 0 , BodyPart:14-(0.71, 0.53) score=0.40\n",
      "human_id= 0 , BodyPart:15-(0.72, 0.53) score=0.59\n",
      "human_id= 0 , BodyPart:17-(0.73, 0.53) score=0.55\n",
      "(720,)\n"
     ]
    }
   ],
   "source": [
    "model_path='mobilenet_thin'\n",
    "resolution = '320x240'\n",
    "showBG=True\n",
    "    \n",
    "logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "w, h = model_wh(resolution)\n",
    "e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
    "\n",
    "\n",
    "video = '../UCF-101/PlayingPiano/v_PlayingPiano_g02_c01.avi'\n",
    "cap = cv2.VideoCapture(video)\n",
    "\n",
    "#---------------modified----------------#\n",
    "num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print (\"All Frames: \" ,num_frames)\n",
    "cur_frames = 0.0\n",
    "step = (num_frames / 20.0) \n",
    "#---------------modified----------------#\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "resize_out_ratio = 8.0\n",
    "print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "single_video_features = np.array([])\n",
    "if cap.isOpened() is False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "while (cap.isOpened()):   \n",
    "    if(cur_frames >= num_frames):\n",
    "        break\n",
    "\n",
    "    frame_no = (cur_frames/num_frames)\n",
    "    cap.set(1,frame_no)\n",
    "    ret_val, image = cap.read()\n",
    "    \n",
    "    print(\"Frame no: \", frame_no)\n",
    "    print (\"Count: \", cur_frames)\n",
    "    \n",
    "    if ret_val == True:\n",
    "        humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "        #print (\"Frame numbers: \", cur_frames, humans)\n",
    "        frame_feature = process_human_data(humans) \n",
    "        single_video_features = np.append(single_video_features, frame_feature) \n",
    "        \n",
    "    cur_frames+=step\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "print (single_video_features)\n",
    "cv2.destroyAllWindows()\n",
    "logger.debug('finished+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.475      0.22916667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature = np.zeros(shape=(18,2))\n",
    "for i in range(18):\n",
    "    if i not in humans[0].body_parts:\n",
    "        feature[i] = [0, 0]\n",
    "    else:\n",
    "        feature[i] = [bp[i].x, bp[i].y]\n",
    "feature = list(feature)\n",
    "\n",
    "print(feature[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:09:30,102] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:09:30,103] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n",
      "[2018-07-25 17:09:54,016] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:09:54,017] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:10:17,711] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:10:17,714] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:10:42,739] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:10:42,740] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:11:06,263] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:11:06,264] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:11:31,947] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:11:31,950] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:11:57,200] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:11:57,202] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:12:21,871] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:12:21,877] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:12:48,201] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:12:48,203] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:13:14,527] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:13:14,530] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:13:40,218] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:13:40,219] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:14:06,991] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:14:06,992] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:14:33,612] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:14:33,615] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:15:00,465] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:15:00,469] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:15:28,082] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:15:28,085] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:15:56,672] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:15:56,675] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:16:25,485] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:16:25,488] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:16:54,925] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:16:54,926] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:17:25,975] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:17:25,978] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:17:58,504] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:17:58,506] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:18:31,706] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:18:31,710] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:19:04,690] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:19:04,693] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:19:38,535] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:19:38,540] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:20:14,202] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:20:14,204] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:20:51,017] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:20:51,019] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:21:29,341] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:21:29,344] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:22:09,732] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:22:09,736] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:22:51,391] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:22:51,394] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:23:34,648] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:23:34,651] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:24:20,872] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:24:20,874] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:25:10,019] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:25:10,023] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:26:01,915] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:26:01,918] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:26:56,230] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:26:56,233] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:27:53,606] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:27:53,609] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:28:55,749] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:28:55,751] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:30:01,727] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:30:01,731] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:31:05,444] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:31:05,446] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:32:13,293] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:32:13,297] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:33:25,461] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:33:25,463] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:34:50,249] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:34:50,253] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:36:20,851] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:36:20,853] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:37:55,820] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:37:55,822] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:39:37,505] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:39:37,507] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:41:26,235] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:41:26,237] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:43:22,344] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:43:22,348] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:45:25,155] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:45:25,158] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:47:36,030] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:47:36,034] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:49:56,522] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:49:56,526] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:52:25,371] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:52:25,375] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:55:02,673] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:55:02,676] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 17:57:49,097] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 17:57:49,101] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:00:43,795] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:00:43,797] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:03:50,123] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:03:50,125] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:07:07,011] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:07:07,015] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:10:37,559] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:10:37,561] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:14:18,259] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:14:18,264] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:18:13,202] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:18:13,205] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:22:15,553] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:22:15,560] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:26:37,173] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:26:37,176] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:31:13,541] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:31:13,546] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 18:36:03,529] [TfPoseEstimator-Video] [DEBUG] initialization mobilenet_thin : /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb\n",
      "[2018-07-25 18:36:03,533] [TfPoseEstimator] [INFO] loading graph from /home/MPLab/mplab006/tf_action_recognition/models/graph/mobilenet_thin/graph_opt.pb(default size=320x240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish single video\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fdfd45f9e9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_set.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0miterate_interest_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fdfd45f9e9c5>\u001b[0m in \u001b[0;36miterate_interest_dir\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mabs_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minference_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f3d877a9476>\u001b[0m in \u001b[0;36minference_video\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initialization %s : %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_graph_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_wh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfPoseEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_graph_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_action_recognition/tf_pose/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph_path, target_size, tf_config)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# warm-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         self.persistent_sess.run(tf.variables_initializer(\n\u001b[0;32m--> 180\u001b[0;31m             [v for v in tf.global_variables() if\n\u001b[0m\u001b[1;32m    181\u001b[0m              v.name.split(':')[0] in [x.decode('utf-8') for x in\n\u001b[1;32m    182\u001b[0m                                       self.persistent_sess.run(tf.report_uninitialized_variables())]\n",
      "\u001b[0;32m~/tf_action_recognition/tf_pose/estimator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m             [v for v in tf.global_variables() if\n\u001b[1;32m    181\u001b[0m              v.name.split(':')[0] in [x.decode('utf-8') for x in\n\u001b[0;32m--> 182\u001b[0;31m                                       self.persistent_sess.run(tf.report_uninitialized_variables())]\n\u001b[0m\u001b[1;32m    183\u001b[0m              ])\n\u001b[1;32m    184\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "video_dict = { 'PlayingCello':0,'PlayingDaf':1,'PlayingDhol':2,'PlayingFlute':3,'PlayingGuitar':4,'PlayingPiano':5, 'PlayingSitar':6,'PlayingTabla':7,'PlayingViolin':8}\n",
    "rootdir = '/home/MPLab/mplab006/UCF-101/'\n",
    "def iterate_interest_dir():\n",
    "    feature_set=[]\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for dirss in dirs:\n",
    "            if (dirss in video_dict):\n",
    "                for filename in os.listdir(os.path.join(subdir,dirss)):\n",
    "                    abs_path =os.path.join(subdir,dirss,filename)\n",
    "                    feature =inference_video(abs_path)\n",
    "                    classification = get_classification(dirss)\n",
    "                    feature =list(feature)\n",
    "                    print(\"finish single video\")\n",
    "                    feature_set.append([feature,classification])\n",
    "                    #print(feature_set)\n",
    "    with open('feature_set.pickle','wb') as file:\n",
    "        pickle.dump(feature_set,file)\n",
    "iterate_interest_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(filename):\n",
    "    label=np.zeros(shape=(9))\n",
    "    label[video_dict[filename]]=1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_video(path):\n",
    "    model_path='mobilenet_thin'\n",
    "    resolution = '320x240'\n",
    "    showBG=True\n",
    "\n",
    "    logger.debug('initialization %s : %s' % (model_path, get_graph_path(model_path)))\n",
    "    w, h = model_wh(resolution)\n",
    "    e = TfPoseEstimator(get_graph_path(model_path), target_size=(w, h))\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    #---------------modified----------------#\n",
    "    num_frames = float(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print (\"All Frames: \" ,num_frames)\n",
    "    cur_frames = 0.0\n",
    "    step = (num_frames / 20.0) \n",
    "    #---------------modified----------------#\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    resize_out_ratio = 8.0\n",
    "    #print(\"Image Size: %d x %d\" % (width, height)) \n",
    "\n",
    "    single_video_features = np.array([])\n",
    "    if cap.isOpened() is False:\n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    while (cap.isOpened()):   \n",
    "        if(cur_frames >= num_frames):\n",
    "            break\n",
    "\n",
    "        frame_no = (cur_frames/num_frames)\n",
    "        cap.set(1,frame_no)\n",
    "        ret_val, image = cap.read()\n",
    "\n",
    "        #print(\"Frame no: \", frame_no)\n",
    "        #print (\"Count: \", cur_frames)\n",
    "\n",
    "        if ret_val == True:\n",
    "            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "            #print (\"Frame numbers: \", cur_frames, humans)\n",
    "            frame_feature = process_human_data(humans) \n",
    "            single_video_features = np.append(single_video_features, frame_feature) \n",
    "\n",
    "        cur_frames+=step\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    #print (single_video_features)\n",
    "    cv2.destroyAllWindows()\n",
    "    #logger.debug('finished+')\n",
    "    return single_video_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
